# Z-Imageæ¨¡å‹æŠ€æœ¯ç©¿åˆºè®¡åˆ’

**æ–‡æ¡£ç±»å‹**: æŠ€æœ¯ç©¿åˆºå®æ–½æ‰‹å†Œ
**åˆ›å»ºæ—¥æœŸ**: 2026-01-08
**é¢„è®¡å‘¨æœŸ**: 1å‘¨
**ç›®æ ‡**: å¿«é€ŸéªŒè¯Z-Imageåœ¨appå¼‚å¸¸ç•Œé¢ç”Ÿæˆä¸­çš„å¯è¡Œæ€§

---

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ€æœ¯ç©¿åˆºæ—¨åœ¨éªŒè¯Z-Image Turboå’ŒZ-Image-Editæ¨¡å‹åœ¨ç”Ÿæˆappå¼‚å¸¸ç•Œé¢æˆªå›¾æ–¹é¢çš„èƒ½åŠ›ï¼Œä¸ºåç»­å¤§è§„æ¨¡åº”ç”¨æä¾›å†³ç­–ä¾æ®ã€‚

### æ ¸å¿ƒéªŒè¯ç‚¹

1. âœ… **æ–‡ç”Ÿå›¾èƒ½åŠ›**: ä»æ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆappå¼‚å¸¸ç•Œé¢
2. âœ… **å›¾åƒç¼–è¾‘èƒ½åŠ›**: åœ¨æ­£å¸¸ç•Œé¢ä¸Šæ³¨å…¥å¼‚å¸¸å…ƒç´ 
3. âœ… **è´¨é‡è¯„ä¼°**: ç”Ÿæˆå›¾åƒçš„å¯ç”¨æ€§å’ŒçœŸå®æ„Ÿ
4. âœ… **æ€§èƒ½è¡¨ç°**: åœ¨ç°æœ‰GPUä¸Šçš„è¿è¡Œæ•ˆç‡

### æµ‹è¯•åœºæ™¯

- ğŸ” **å¤–å–ç±»**: ç¾å›¢ï¼ˆå•†å“ç¼ºè´§ã€å¹¿å‘Šé®æŒ¡ã€é…é€è¶…æ—¶ï¼‰
- ğŸ’° **æ”¯ä»˜ç±»**: æ”¯ä»˜å®/å¾®ä¿¡ï¼ˆä½™é¢ä¸è¶³ã€æ”¯ä»˜è¶…æ—¶ã€ç½‘ç»œé”™è¯¯ï¼‰
- ğŸš— **å‡ºè¡Œç±»**: æºç¨‹/æ»´æ»´ï¼ˆä½™ç¥¨ä¸º0ã€æ— å¯ç”¨è½¦è¾†ã€ä»·æ ¼å¼‚å¸¸ï¼‰

---

## Day 0: ç¯å¢ƒå‡†å¤‡

### ç¡¬ä»¶è¦æ±‚

**ç°æœ‰ç¡¬ä»¶**: RTX 3090/4080ç­‰ï¼ˆé4090ï¼‰

**ä¼˜åŒ–ç­–ç•¥**:
```bash
# æ ¹æ®æ˜¾å­˜è°ƒæ•´ç­–ç•¥ï¼š
# RTX 3090 (24GB): å¯è¿è¡Œå®Œæ•´Z-Image Turbo
# RTX 4080 (16GB): éœ€è¦é€‚å½“ä¼˜åŒ–
# RTX 3080 (10-12GB): éœ€è¦é™ä½åˆ†è¾¨ç‡æˆ–ä½¿ç”¨é‡åŒ–
```

### è½¯ä»¶ç¯å¢ƒæ­å»º

#### Step 1: åˆ›å»ºPythonç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n z-image python=3.10 -y
conda activate z-image

# å®‰è£…PyTorch (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Step 2: å®‰è£…æ ¸å¿ƒä¾èµ–

```bash
# å®‰è£…Hugging Faceç”Ÿæ€
pip install transformers diffusers accelerate
pip install safetensors
pip install pillow opencv-python

# å®‰è£…è¯„ä¼°å·¥å…·
pip install lpips clip scikit-image
pip install gradio  # ç”¨äºå¿«é€Ÿæ„å»ºæµ‹è¯•ç•Œé¢

# å¯é€‰ï¼šå®‰è£…xFormersåŠ é€Ÿæ¨ç†
pip install xformers
```

#### Step 3: ä¸‹è½½æ¨¡å‹

```python
# download_models.py
from huggingface_hub import snapshot_download
import os

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = "./models"
os.makedirs(cache_dir, exist_ok=True)

# ä¸‹è½½Z-Image Turboï¼ˆå‡è®¾æ¨¡å‹åç§°ï¼‰
# æ³¨æ„ï¼šå®é™…æ¨¡å‹åç§°éœ€è¦ä»Hugging FaceæŸ¥æ‰¾
models_to_download = [
    # "stabilityai/z-image-turbo",  # æ–‡ç”Ÿå›¾åŸºç¡€æ¨¡å‹
    # "stabilityai/z-image-edit",    # å›¾åƒç¼–è¾‘æ¨¡å‹
]

print("æ³¨æ„ï¼šè¯·æ‰‹åŠ¨ä»Hugging Faceæœç´¢Z-Imageçš„å®é™…æ¨¡å‹åç§°")
print("æœç´¢å…³é”®è¯ï¼šZ-Image, Z-Image Turbo, Z-Image Edit")
print("æ¨¡å‹ä»“åº“ç¤ºä¾‹ï¼šhttps://huggingface.co/models?search=z-image")

# å¦‚æœæ‰¾åˆ°æ¨¡å‹ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ä¸‹è½½ï¼š
# for model_name in models_to_download:
#     print(f"Downloading {model_name}...")
#     snapshot_download(
#         repo_id=model_name,
#         cache_dir=cache_dir,
#         resume_download=True
#     )
```

**æ‰§è¡Œä¸‹è½½**:
```bash
cd prototypes
python download_models.py
```

### é¡¹ç›®ç»“æ„

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p prototypes/z_image_spike
cd prototypes/z_image_spike

mkdir -p {data,outputs,scripts,notebooks}
mkdir -p data/{reference_images,test_prompts}
mkdir -p outputs/{text2img,img2img,comparisons}
```

**ç›®å½•è¯´æ˜**:
- `data/reference_images/`: å­˜æ”¾æ­£å¸¸appæˆªå›¾ï¼ˆç”¨äºç¼–è¾‘æµ‹è¯•ï¼‰
- `data/test_prompts/`: æµ‹è¯•æç¤ºè¯æ–‡ä»¶
- `outputs/text2img/`: æ–‡ç”Ÿå›¾ç»“æœ
- `outputs/img2img/`: å›¾åƒç¼–è¾‘ç»“æœ
- `outputs/comparisons/`: å¯¹æ¯”è¯„ä¼°ç»“æœ
- `scripts/`: Pythonè„šæœ¬
- `notebooks/`: Jupyterå®éªŒç¬”è®°æœ¬

### æ•°æ®å‡†å¤‡

#### é‡‡é›†å‚è€ƒå›¾åƒ

```bash
# æ‰‹åŠ¨é‡‡é›†æˆ–ä½¿ç”¨ä»¥ä¸‹å·¥å…·
# æ–¹å¼1ï¼šæ‰‹æœºæˆªå›¾ä¼ è¾“
adb devices  # æ£€æŸ¥Androidè®¾å¤‡è¿æ¥
adb pull /sdcard/Screenshots/*.png data/reference_images/

# æ–¹å¼2ï¼šä½¿ç”¨ç°æœ‰å›¾åº“
# ä»ç½‘ç»œæœç´¢"ç¾å›¢appç•Œé¢"ã€"æ”¯ä»˜å®ç•Œé¢"ç­‰å…³é”®è¯
# æˆ–ä½¿ç”¨å…¬å¼€æ•°æ®é›†
```

**æ•°æ®æ¸…å•** (æ¯ç±»appå‡†å¤‡5-10å¼ ):
- `meituan_normal_*.png`: ç¾å›¢æ­£å¸¸ç•Œé¢
- `alipay_normal_*.png`: æ”¯ä»˜å®æ­£å¸¸ç•Œé¢
- `ctrip_normal_*.png`: æºç¨‹æ­£å¸¸ç•Œé¢

---

## Day 1-2: åŸºç¡€æ¨ç†æµ‹è¯•

### ç›®æ ‡
éªŒè¯æ¨¡å‹èƒ½å¦æ­£å¸¸åŠ è½½å’Œè¿è¡Œï¼Œæµ‹è¯•åŸºæœ¬çš„æ–‡ç”Ÿå›¾èƒ½åŠ›

### Step 1: ç®€å•æ¨ç†è„šæœ¬

åˆ›å»º `scripts/test_basic_inference.py`:

```python
"""
åŸºç¡€æ¨ç†æµ‹è¯•è„šæœ¬
éªŒè¯Z-Imageæ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ
"""
import torch
from diffusers import DiffusionPipeline
from PIL import Image
import os

# é…ç½®
MODEL_PATH = "path/to/z-image-turbo"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
OUTPUT_DIR = "outputs/text2img"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ˜¾å­˜ä¼˜åŒ–é…ç½®
ENABLE_XFORMERS = True
ENABLE_CPU_OFFLOAD = False  # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œè®¾ä¸ºTrue
USE_FP16 = True

def load_model():
    """åŠ è½½Z-Imageæ¨¡å‹"""
    print("Loading model...")

    # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´
    if USE_FP16:
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32

    pipe = DiffusionPipeline.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch_dtype,
        use_safetensors=True
    )

    # ä¼˜åŒ–è®¾ç½®
    if ENABLE_XFORMERS:
        pipe.enable_xformers_memory_efficient_attention()

    if ENABLE_CPU_OFFLOAD:
        pipe.enable_model_cpu_offload()
    else:
        pipe = pipe.to("cuda")

    print(f"Model loaded. Using dtype: {torch_dtype}")
    return pipe

def generate_test_image(pipe, prompt, seed=42):
    """ç”Ÿæˆæµ‹è¯•å›¾åƒ"""
    generator = torch.Generator(device="cuda").manual_seed(seed)

    # æ ¹æ®æ˜¾å­˜è°ƒæ•´åˆ†è¾¨ç‡
    # 24GB: 1024x1024
    # 16GB: 768x768 æˆ– 512x512
    # 12GB: 512x512

    image = pipe(
        prompt=prompt,
        num_inference_steps=20,  # Turboç‰ˆæœ¬é€šå¸¸éœ€è¦è¾ƒå°‘æ­¥æ•°
        generator=generator,
        height=512,
        width=512,
    ).images[0]

    return image

def main():
    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "A mobile app screenshot showing a food delivery interface",
        "ç¾å›¢å¤–å–appç•Œé¢ï¼Œæ˜¾ç¤ºå•†å“åˆ—è¡¨",
        "A payment app showing insufficient balance error",
    ]

    # åŠ è½½æ¨¡å‹
    pipe = load_model()

    # ç”Ÿæˆæµ‹è¯•å›¾åƒ
    for i, prompt in enumerate(test_prompts):
        print(f"\nGenerating image {i+1}/{len(test_prompts)}")
        print(f"Prompt: {prompt}")

        image = generate_test_image(pipe, prompt, seed=42+i)

        output_path = f"{OUTPUT_DIR}/test_{i+1}.png"
        image.save(output_path)
        print(f"Saved to {output_path}")

    print("\nâœ… Basic inference test completed!")

if __name__ == "__main__":
    main()
```

### Step 2: è¿è¡ŒåŸºç¡€æµ‹è¯•

```bash
cd prototypes/z_image_spike
python scripts/test_basic_inference.py
```

### Step 3: æ€§èƒ½åŸºå‡†æµ‹è¯•

åˆ›å»º `scripts/benchmark.py`:

```python
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹é‡ç”Ÿæˆé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨
"""
import torch
import time
from test_basic_inference import load_model, generate_test_image

def benchmark():
    pipe = load_model()

    test_prompt = "A mobile app interface screenshot"

    # é¢„çƒ­
    print("Warming up...")
    _ = generate_test_image(pipe, test_prompt)

    # æ­£å¼æµ‹è¯•
    print("\nRunning benchmark...")
    times = []
    for i in range(5):
        torch.cuda.synchronize()
        start = time.time()

        _ = generate_test_image(pipe, test_prompt, seed=i)

        torch.cuda.synchronize()
        end = time.time()

        elapsed = end - start
        times.append(elapsed)
        print(f"Run {i+1}: {elapsed:.2f}s")

    # æ˜¾å­˜ç»Ÿè®¡
    memory_allocated = torch.cuda.max_memory_allocated() / 1024**3
    print(f"\nğŸ“Š Performance Summary:")
    print(f"  Average time: {sum(times)/len(times):.2f}s")
    print(f"  Min time: {min(times):.2f}s")
    print(f"  Max time: {max(times):.2f}s")
    print(f"  Peak memory: {memory_allocated:.2f} GB")

if __name__ == "__main__":
    benchmark()
```

**é¢„æœŸç»“æœ**:
- âœ… æ¨¡å‹æˆåŠŸåŠ è½½
- âœ… ç”Ÿæˆé€Ÿåº¦: 2-5ç§’/å›¾ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
- âœ… æ˜¾å­˜å ç”¨: 8-12GBï¼ˆFP16æ¨¡å¼ï¼‰

---

## Day 3-4: æ–‡ç”Ÿå›¾èƒ½åŠ›éªŒè¯

### ç›®æ ‡
æµ‹è¯•Z-Imageä»æ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆappå¼‚å¸¸ç•Œé¢çš„èƒ½åŠ›

### æµ‹è¯•ç”¨ä¾‹è®¾è®¡

åˆ›å»º `data/test_prompts/anomaly_prompts.json`:

```json
{
  "meituan": [
    {
      "id": "mt_001",
      "category": "out_of_stock",
      "prompt": "ç¾å›¢å¤–å–appç•Œé¢æˆªå›¾ï¼Œæ˜¾ç¤ºä¸€ä¸ªé¤å…èœå“é¡µé¢ï¼Œå¤šä¸ªèœå“æ˜¾ç¤ºçº¢è‰²çš„"å·²å”®ç½„"æ ‡ç­¾ï¼Œç•Œé¢é£æ ¼ä¸ºç¾å›¢æ ‡å‡†çš„é»„è‰²ä¸»é¢˜",
      "prompt_en": "Meituan food delivery app screenshot, restaurant menu page, multiple dishes showing red 'sold out' tags, yellow Meituan theme"
    },
    {
      "id": "mt_002",
      "category": "ad_blocking",
      "prompt": "ç¾å›¢å¤–å–appä¸»ç•Œé¢ï¼Œä¸­å¤®å‡ºç°ä¸€ä¸ªåŠé€æ˜çš„å…¨å±å¹¿å‘Šå¼¹çª—ï¼Œé®æŒ¡ä½ä¸‹æ–¹çš„é¤å…åˆ—è¡¨ï¼Œå¹¿å‘Šå†…å®¹ä¸ºä¿ƒé”€æ´»åŠ¨",
      "prompt_en": "Meituan app home screen with a semi-transparent full-screen promotional ad popup blocking the restaurant list below"
    },
    {
      "id": "mt_003",
      "category": "delivery_delay",
      "prompt": "ç¾å›¢å¤–å–è®¢å•è¯¦æƒ…é¡µé¢ï¼Œé¡¶éƒ¨æ˜¾ç¤ºçº¢è‰²è­¦å‘Šæ¨ªå¹…æç¤º"é…é€å¼‚å¸¸ï¼Œé¢„è®¡å»¶è¿Ÿ30åˆ†é’Ÿ"ï¼Œä¸‹æ–¹æ˜¯è®¢å•ä¿¡æ¯",
      "prompt_en": "Meituan order details page with red warning banner at top showing 'Delivery delayed by 30 minutes', order info below"
    }
  ],
  "alipay": [
    {
      "id": "ap_001",
      "category": "insufficient_balance",
      "prompt": "æ”¯ä»˜å®æ”¯ä»˜ç•Œé¢ï¼Œæ˜¾ç¤ºçº¢è‰²é”™è¯¯æç¤º"è´¦æˆ·ä½™é¢ä¸è¶³"ï¼Œä½™é¢æ˜¾ç¤ºä¸ºÂ¥0.50ï¼Œæ”¯ä»˜é‡‘é¢ä¸ºÂ¥58.00",
      "prompt_en": "Alipay payment screen showing red error 'Insufficient balance', balance Â¥0.50, payment amount Â¥58.00"
    },
    {
      "id": "ap_002",
      "category": "payment_timeout",
      "prompt": "æ”¯ä»˜å®ç•Œé¢ï¼Œä¸­å¤®æ˜¾ç¤ºä¸€ä¸ªç°è‰²çš„è¶…æ—¶å›¾æ ‡ï¼Œä¸‹æ–¹æ–‡å­—"æ”¯ä»˜è¶…æ—¶ï¼Œè¯·é‡è¯•"ï¼ŒèƒŒæ™¯ä¸ºæ”¯ä»˜å®è“è‰²ä¸»é¢˜",
      "prompt_en": "Alipay interface with gray timeout icon in center, text 'Payment timeout, please retry', blue Alipay theme"
    },
    {
      "id": "ap_003",
      "category": "network_error",
      "prompt": "æ”¯ä»˜å®é¡µé¢æ˜¾ç¤ºç½‘ç»œæ–­å¼€å›¾æ ‡ï¼Œæç¤º"ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®"ï¼Œé¡¶éƒ¨çŠ¶æ€æ æ˜¾ç¤ºæ— ç½‘ç»œä¿¡å·",
      "prompt_en": "Alipay page showing network disconnection icon, message 'Network connection failed', no signal in status bar"
    }
  ],
  "ctrip": [
    {
      "id": "ct_001",
      "category": "no_tickets",
      "prompt": "æºç¨‹ç«è½¦ç¥¨æŸ¥è¯¢ç»“æœé¡µé¢ï¼Œæ˜¾ç¤º"æ— ç¥¨"çš„ç°è‰²æ ‡ç­¾ï¼Œå¤šä¸ªè½¦æ¬¡éƒ½æ˜¾ç¤ºå·²å”®ç½„çŠ¶æ€",
      "prompt_en": "Ctrip train ticket search results showing gray 'No tickets' labels, multiple trains sold out"
    },
    {
      "id": "ct_002",
      "category": "no_vehicles",
      "prompt": "æ»´æ»´æ‰“è½¦ç•Œé¢ï¼Œåœ°å›¾ä¸­å¤®æ˜¾ç¤º"é™„è¿‘æš‚æ— å¯ç”¨è½¦è¾†"çš„æç¤ºï¼Œåœ°å›¾ä¸Šæ²¡æœ‰è½¦è¾†å›¾æ ‡",
      "prompt_en": "DiDi ride-hailing interface, map center showing 'No vehicles available nearby', no car icons on map"
    },
    {
      "id": "ct_003",
      "category": "price_surge",
      "prompt": "æºç¨‹é…’åº—é¢„è®¢é¡µé¢ï¼Œä»·æ ¼æ˜¾ç¤ºä¸ºçº¢è‰²ï¼Œæ—è¾¹æœ‰"ä»·æ ¼å¼‚å¸¸ä¸Šæ¶¨"çš„è­¦å‘Šå›¾æ ‡å’Œæ–‡å­—",
      "prompt_en": "Ctrip hotel booking page, price in red with warning icon and text 'Abnormal price increase'"
    }
  ]
}
```

### æ‰¹é‡ç”Ÿæˆè„šæœ¬

åˆ›å»º `scripts/generate_from_text.py`:

```python
"""
æ–‡ç”Ÿå›¾æ‰¹é‡æµ‹è¯•è„šæœ¬
ä»æç¤ºè¯ç”Ÿæˆappå¼‚å¸¸ç•Œé¢
"""
import torch
from diffusers import DiffusionPipeline
from PIL import Image
import json
import os
from pathlib import Path

# é…ç½®
MODEL_PATH = "path/to/z-image-turbo"
PROMPTS_FILE = "data/test_prompts/anomaly_prompts.json"
OUTPUT_DIR = "outputs/text2img"

def load_prompts():
    """åŠ è½½æµ‹è¯•æç¤ºè¯"""
    with open(PROMPTS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def generate_batch(pipe, prompts_data):
    """æ‰¹é‡ç”Ÿæˆå›¾åƒ"""

    for app_name, prompts in prompts_data.items():
        print(f"\n{'='*50}")
        print(f"Processing {app_name.upper()}")
        print(f"{'='*50}")

        app_output_dir = os.path.join(OUTPUT_DIR, app_name)
        os.makedirs(app_output_dir, exist_ok=True)

        for prompt_item in prompts:
            prompt_id = prompt_item["id"]
            category = prompt_item["category"]
            prompt = prompt_item["prompt"]
            prompt_en = prompt_item.get("prompt_en", "")

            print(f"\n[{prompt_id}] {category}")
            print(f"Prompt: {prompt[:50]}...")

            # å°è¯•ä¸­æ–‡prompt
            try:
                image = pipe(
                    prompt=prompt,
                    negative_prompt="blurry, low quality, distorted, watermark",
                    num_inference_steps=20,
                    guidance_scale=7.5,
                    height=768,
                    width=512,  # æ‰‹æœºç«–å±æ¯”ä¾‹
                ).images[0]

                output_path = os.path.join(app_output_dir, f"{prompt_id}_cn.png")
                image.save(output_path)
                print(f"  âœ… Saved (Chinese prompt): {output_path}")

            except Exception as e:
                print(f"  âŒ Error with Chinese prompt: {e}")

            # å¦‚æœæœ‰è‹±æ–‡promptï¼Œä¹Ÿå°è¯•ç”Ÿæˆ
            if prompt_en:
                try:
                    image = pipe(
                        prompt=prompt_en,
                        negative_prompt="blurry, low quality, distorted, watermark",
                        num_inference_steps=20,
                        guidance_scale=7.5,
                        height=768,
                        width=512,
                    ).images[0]

                    output_path = os.path.join(app_output_dir, f"{prompt_id}_en.png")
                    image.save(output_path)
                    print(f"  âœ… Saved (English prompt): {output_path}")

                except Exception as e:
                    print(f"  âŒ Error with English prompt: {e}")

def main():
    # åŠ è½½æ¨¡å‹
    print("Loading Z-Image Turbo model...")
    pipe = DiffusionPipeline.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        use_safetensors=True
    ).to("cuda")

    pipe.enable_xformers_memory_efficient_attention()

    # åŠ è½½æç¤ºè¯
    prompts_data = load_prompts()

    # æ‰¹é‡ç”Ÿæˆ
    generate_batch(pipe, prompts_data)

    print("\n" + "="*50)
    print("âœ… Text-to-image generation completed!")
    print(f"Check results in: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
```

### è¿è¡Œç”Ÿæˆ

```bash
python scripts/generate_from_text.py
```

### è´¨é‡è¯„ä¼°ï¼ˆäººå·¥ï¼‰

åˆ›å»ºè¯„ä¼°è¡¨æ ¼ `outputs/text2img/evaluation.md`:

```markdown
# æ–‡ç”Ÿå›¾è´¨é‡è¯„ä¼°

è¯„åˆ†æ ‡å‡†ï¼š
- 5åˆ†ï¼šå®Œç¾ï¼Œå®Œå…¨ç¬¦åˆé¢„æœŸ
- 4åˆ†ï¼šä¼˜ç§€ï¼Œè½»å¾®ç‘•ç–µ
- 3åˆ†ï¼šåˆæ ¼ï¼ŒåŸºæœ¬å¯ç”¨
- 2åˆ†ï¼šè¾ƒå·®ï¼Œéœ€è¦æ”¹è¿›
- 1åˆ†ï¼šå¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨

| ID | App | å¼‚å¸¸ç±»å‹ | ç•Œé¢é£æ ¼ | å¼‚å¸¸å…ƒç´  | æ–‡å­—æ¸…æ™°åº¦ | æ•´ä½“çœŸå®æ„Ÿ | æ€»åˆ† | å¤‡æ³¨ |
|----|-----|---------|---------|---------|-----------|-----------|------|------|
| mt_001 | ç¾å›¢ | ç¼ºè´§ | /5 | /5 | /5 | /5 | /20 | |
| mt_002 | ç¾å›¢ | å¹¿å‘Šé®æŒ¡ | /5 | /5 | /5 | /5 | /20 | |
| ... | ... | ... | ... | ... | ... | ... | ... | ... |

**å…³é”®è§‚å¯Ÿç‚¹**:
1. æ–‡å­—æ˜¯å¦æ¸…æ™°å¯è¾¨è®¤ï¼ˆä¸­æ–‡æ”¯æŒï¼‰
2. UIæ§ä»¶ï¼ˆæŒ‰é’®ã€è¾“å…¥æ¡†ï¼‰æ˜¯å¦çœŸå®
3. é¢œè‰²é£æ ¼æ˜¯å¦ç¬¦åˆç›®æ ‡app
4. å¼‚å¸¸å…ƒç´ ä½ç½®æ˜¯å¦åˆç†
5. æ•´ä½“å¸ƒå±€æ˜¯å¦åè°ƒ
```

---

## Day 5-6: å›¾åƒç¼–è¾‘èƒ½åŠ›éªŒè¯

### ç›®æ ‡
æµ‹è¯•Z-Image-Editåœ¨æ­£å¸¸ç•Œé¢ä¸Šæ³¨å…¥å¼‚å¸¸å…ƒç´ çš„èƒ½åŠ›

### Step 1: å‡†å¤‡ç¼–è¾‘ä»»åŠ¡

åˆ›å»º `data/test_prompts/edit_tasks.json`:

```json
{
  "edit_tasks": [
    {
      "id": "edit_001",
      "source_image": "data/reference_images/meituan_normal_1.png",
      "edit_instruction": "åœ¨å•†å“åˆ—è¡¨çš„ç¬¬äºŒä¸ªå•†å“ä¸Šæ·»åŠ çº¢è‰²çš„"å·²å”®ç½„"æ ‡ç­¾",
      "expected_result": "å•†å“æ˜¾ç¤ºç¼ºè´§çŠ¶æ€"
    },
    {
      "id": "edit_002",
      "source_image": "data/reference_images/meituan_normal_1.png",
      "edit_instruction": "åœ¨ç•Œé¢ä¸­å¤®æ·»åŠ ä¸€ä¸ªåŠé€æ˜çš„ä¿ƒé”€å¹¿å‘Šå¼¹çª—ï¼Œé®æŒ¡éƒ¨åˆ†å†…å®¹",
      "expected_result": "å¹¿å‘Šé®æŒ¡ç•Œé¢"
    },
    {
      "id": "edit_003",
      "source_image": "data/reference_images/alipay_normal_1.png",
      "edit_instruction": "åœ¨ä½™é¢æ•°å­—ä½ç½®æ˜¾ç¤ºçº¢è‰²çš„"ä½™é¢ä¸è¶³"é”™è¯¯æç¤º",
      "expected_result": "æ˜¾ç¤ºä½™é¢ä¸è¶³å¼‚å¸¸"
    },
    {
      "id": "edit_004",
      "source_image": "data/reference_images/ctrip_normal_1.png",
      "edit_instruction": "å°†ç¥¨ä»·æ—è¾¹çš„"æœ‰ç¥¨"æ”¹ä¸ºç°è‰²çš„"æ— ç¥¨"ï¼Œå¹¶ç¦ç”¨è´­ä¹°æŒ‰é’®",
      "expected_result": "æ˜¾ç¤ºæ— ç¥¨çŠ¶æ€"
    }
  ]
}
```

### Step 2: å›¾åƒç¼–è¾‘è„šæœ¬

åˆ›å»º `scripts/edit_images.py`:

```python
"""
å›¾åƒç¼–è¾‘æµ‹è¯•è„šæœ¬
ä½¿ç”¨Z-Image-Editåœ¨æ­£å¸¸ç•Œé¢ä¸Šæ³¨å…¥å¼‚å¸¸
"""
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline
from PIL import Image
import json
import os

# é…ç½®
EDIT_MODEL_PATH = "path/to/z-image-edit"  # æˆ–ä½¿ç”¨InstructPix2Pixä½œä¸ºæ›¿ä»£
TASKS_FILE = "data/test_prompts/edit_tasks.json"
OUTPUT_DIR = "outputs/img2img"

def load_edit_model():
    """åŠ è½½å›¾åƒç¼–è¾‘æ¨¡å‹"""
    print("Loading image editing model...")

    # å¦‚æœZ-Image-Editä¸å¯ç”¨ï¼Œå¯ä»¥ä½¿ç”¨InstructPix2Pixä½œä¸ºæ›¿ä»£
    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
        "timbrooks/instruct-pix2pix",  # å¤‡ç”¨æ–¹æ¡ˆ
        torch_dtype=torch.float16,
        safety_checker=None
    ).to("cuda")

    pipe.enable_xformers_memory_efficient_attention()

    return pipe

def edit_image(pipe, source_path, instruction, output_path):
    """ç¼–è¾‘å•å¼ å›¾åƒ"""
    # åŠ è½½æºå›¾åƒ
    image = Image.open(source_path).convert("RGB")

    # æ‰§è¡Œç¼–è¾‘
    edited = pipe(
        prompt=instruction,
        image=image,
        num_inference_steps=20,
        image_guidance_scale=1.5,
        guidance_scale=7.5,
    ).images[0]

    # ä¿å­˜ç»“æœ
    edited.save(output_path)

    # åˆ›å»ºå¯¹æ¯”å›¾
    comparison = Image.new('RGB', (image.width * 2, image.height))
    comparison.paste(image, (0, 0))
    comparison.paste(edited, (image.width, 0))
    comparison_path = output_path.replace('.png', '_comparison.png')
    comparison.save(comparison_path)

    return edited, comparison_path

def main():
    # åŠ è½½æ¨¡å‹
    pipe = load_edit_model()

    # åŠ è½½ç¼–è¾‘ä»»åŠ¡
    with open(TASKS_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # æ‰§è¡Œç¼–è¾‘
    for task in data["edit_tasks"]:
        task_id = task["id"]
        source_path = task["source_image"]
        instruction = task["edit_instruction"]

        print(f"\n[{task_id}]")
        print(f"Source: {source_path}")
        print(f"Instruction: {instruction}")

        if not os.path.exists(source_path):
            print(f"  âš ï¸  Source image not found, skipping...")
            continue

        output_path = os.path.join(OUTPUT_DIR, f"{task_id}.png")

        try:
            edited, comparison_path = edit_image(
                pipe, source_path, instruction, output_path
            )
            print(f"  âœ… Saved: {output_path}")
            print(f"  âœ… Comparison: {comparison_path}")
        except Exception as e:
            print(f"  âŒ Error: {e}")

    print("\nâœ… Image editing completed!")

if __name__ == "__main__":
    main()
```

### Step 3: å¯¹æ¯”ä¸åŒç¼–è¾‘å¼ºåº¦

åˆ›å»º `scripts/edit_strength_comparison.py`:

```python
"""
ç¼–è¾‘å¼ºåº¦å¯¹æ¯”æµ‹è¯•
æµ‹è¯•ä¸åŒå‚æ•°å¯¹ç¼–è¾‘æ•ˆæœçš„å½±å“
"""
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline
from PIL import Image
import os

def compare_edit_strengths():
    """å¯¹æ¯”ä¸åŒç¼–è¾‘å¼ºåº¦"""

    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
        "timbrooks/instruct-pix2pix",
        torch_dtype=torch.float16,
    ).to("cuda")

    # æµ‹è¯•å‚æ•°
    source_image = "data/reference_images/meituan_normal_1.png"
    instruction = "åœ¨ç•Œé¢é¡¶éƒ¨æ·»åŠ ä¸€ä¸ªçº¢è‰²çš„é”™è¯¯æç¤ºæ¨ªå¹…"

    image = Image.open(source_image).convert("RGB")

    # æµ‹è¯•ä¸åŒçš„guidance scale
    scales = [1.0, 1.5, 2.0, 3.0]

    output_dir = "outputs/img2img/strength_comparison"
    os.makedirs(output_dir, exist_ok=True)

    for scale in scales:
        print(f"\nTesting image_guidance_scale={scale}")

        edited = pipe(
            prompt=instruction,
            image=image,
            num_inference_steps=20,
            image_guidance_scale=scale,
            guidance_scale=7.5,
        ).images[0]

        output_path = f"{output_dir}/scale_{scale}.png"
        edited.save(output_path)
        print(f"  Saved: {output_path}")

if __name__ == "__main__":
    compare_edit_strengths()
```

### è¿è¡Œç¼–è¾‘æµ‹è¯•

```bash
# åŸºç¡€ç¼–è¾‘æµ‹è¯•
python scripts/edit_images.py

# å‚æ•°å¯¹æ¯”æµ‹è¯•
python scripts/edit_strength_comparison.py
```

---

## Day 7: è¯„ä¼°ä¸æ€»ç»“

### è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°

åˆ›å»º `scripts/evaluate_quality.py`:

```python
"""
è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°
ä½¿ç”¨CLIPã€LPIPSç­‰æŒ‡æ ‡è¯„ä¼°ç”Ÿæˆè´¨é‡
"""
import torch
from PIL import Image
import clip
import lpips
import os
import json
from pathlib import Path

def load_evaluators():
    """åŠ è½½è¯„ä¼°æ¨¡å‹"""
    # CLIP for semantic similarity
    device = "cuda"
    clip_model, preprocess = clip.load("ViT-B/32", device=device)

    # LPIPS for perceptual similarity
    lpips_model = lpips.LPIPS(net='alex').to(device)

    return clip_model, preprocess, lpips_model, device

def evaluate_text2img(image_path, text_prompt, clip_model, preprocess, device):
    """è¯„ä¼°æ–‡ç”Ÿå›¾è´¨é‡"""
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
    text = clip.tokenize([text_prompt]).to(device)

    with torch.no_grad():
        image_features = clip_model.encode_image(image)
        text_features = clip_model.encode_text(text)

        # Cosine similarity
        similarity = torch.nn.functional.cosine_similarity(
            image_features, text_features
        ).item()

    return similarity

def evaluate_img2img(original_path, edited_path, lpips_model, device):
    """è¯„ä¼°å›¾åƒç¼–è¾‘è´¨é‡"""
    # Load images
    img1 = lpips.im2tensor(lpips.load_image(original_path)).to(device)
    img2 = lpips.im2tensor(lpips.load_image(edited_path)).to(device)

    # Compute distance
    with torch.no_grad():
        distance = lpips_model(img1, img2).item()

    return distance

def main():
    """è¿è¡Œå®Œæ•´è¯„ä¼°"""
    clip_model, preprocess, lpips_model, device = load_evaluators()

    results = {
        "text2img": [],
        "img2img": []
    }

    # è¯„ä¼°æ–‡ç”Ÿå›¾
    print("Evaluating text-to-image results...")
    prompts_file = "data/test_prompts/anomaly_prompts.json"
    with open(prompts_file, 'r', encoding='utf-8') as f:
        prompts_data = json.load(f)

    for app_name, prompts in prompts_data.items():
        for prompt_item in prompts:
            prompt_id = prompt_item["id"]
            prompt = prompt_item["prompt"]
            image_path = f"outputs/text2img/{app_name}/{prompt_id}_cn.png"

            if os.path.exists(image_path):
                score = evaluate_text2img(
                    image_path, prompt, clip_model, preprocess, device
                )
                results["text2img"].append({
                    "id": prompt_id,
                    "app": app_name,
                    "clip_score": score
                })
                print(f"  {prompt_id}: CLIP score = {score:.4f}")

    # è¯„ä¼°å›¾åƒç¼–è¾‘
    print("\nEvaluating image editing results...")
    tasks_file = "data/test_prompts/edit_tasks.json"
    with open(tasks_file, 'r', encoding='utf-8') as f:
        tasks_data = json.load(f)

    for task in tasks_data["edit_tasks"]:
        task_id = task["id"]
        original_path = task["source_image"]
        edited_path = f"outputs/img2img/{task_id}.png"

        if os.path.exists(edited_path) and os.path.exists(original_path):
            distance = evaluate_img2img(
                original_path, edited_path, lpips_model, device
            )
            results["img2img"].append({
                "id": task_id,
                "lpips_distance": distance
            })
            print(f"  {task_id}: LPIPS distance = {distance:.4f}")

    # ä¿å­˜ç»“æœ
    output_file = "outputs/evaluation_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Evaluation completed! Results saved to {output_file}")

    # æ‰“å°æ‘˜è¦
    if results["text2img"]:
        avg_clip = sum(r["clip_score"] for r in results["text2img"]) / len(results["text2img"])
        print(f"\nğŸ“Š Text-to-Image Summary:")
        print(f"   Average CLIP score: {avg_clip:.4f}")
        print(f"   Target: > 0.25 (higher is better)")

    if results["img2img"]:
        avg_lpips = sum(r["lpips_distance"] for r in results["img2img"]) / len(results["img2img"])
        print(f"\nğŸ“Š Image Editing Summary:")
        print(f"   Average LPIPS distance: {avg_lpips:.4f}")
        print(f"   Target: 0.1-0.3 (moderate change)")

if __name__ == "__main__":
    main()
```

### ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

åˆ›å»º `scripts/generate_report.py`:

```python
"""
ç”ŸæˆæŠ€æœ¯ç©¿åˆºæ€»ç»“æŠ¥å‘Š
"""
import json
import os
from pathlib import Path
from datetime import datetime

def generate_html_report():
    """ç”ŸæˆHTMLæ ¼å¼çš„å¯è§†åŒ–æŠ¥å‘Š"""

    # è¯»å–è¯„ä¼°ç»“æœ
    with open("outputs/evaluation_results.json", 'r') as f:
        eval_results = json.load(f)

    # ç»Ÿè®¡å›¾åƒæ•°é‡
    text2img_count = len(list(Path("outputs/text2img").rglob("*.png")))
    img2img_count = len(list(Path("outputs/img2img").rglob("*.png")))

    html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Z-ImageæŠ€æœ¯ç©¿åˆºæŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        h1 {{ color: #2c3e50; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #3498db; color: white; }}
        .metric {{ background-color: #ecf0f1; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .success {{ color: #27ae60; }}
        .warning {{ color: #f39c12; }}
        .fail {{ color: #e74c3c; }}
        img {{ max-width: 400px; margin: 10px; border: 1px solid #ddd; }}
    </style>
</head>
<body>
    <h1>ğŸ¯ Z-Imageæ¨¡å‹æŠ€æœ¯ç©¿åˆºæŠ¥å‘Š</h1>
    <p><strong>æµ‹è¯•æ—¥æœŸ:</strong> {datetime.now().strftime("%Y-%m-%d")}</p>
    <p><strong>æµ‹è¯•ç›®æ ‡:</strong> éªŒè¯Z-Imageåœ¨appå¼‚å¸¸ç•Œé¢ç”Ÿæˆä¸­çš„å¯è¡Œæ€§</p>

    <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
    <div class="metric">
        <p><strong>æ–‡ç”Ÿå›¾æµ‹è¯•:</strong> {text2img_count} å¼ å›¾åƒ</p>
        <p><strong>å›¾åƒç¼–è¾‘æµ‹è¯•:</strong> {img2img_count} å¼ å›¾åƒ</p>
        <p><strong>è¦†ç›–åœºæ™¯:</strong> ç¾å›¢å¤–å–ã€æ”¯ä»˜å®ã€æºç¨‹/æ»´æ»´</p>
    </div>

    <h2>ğŸ”¬ æ–‡ç”Ÿå›¾èƒ½åŠ›è¯„ä¼°</h2>
    <table>
        <tr>
            <th>ID</th>
            <th>App</th>
            <th>CLIP Score</th>
            <th>è¯„ä»·</th>
        </tr>
"""

    for result in eval_results.get("text2img", []):
        score = result["clip_score"]
        rating = "âœ… ä¼˜ç§€" if score > 0.3 else "âš ï¸  ä¸€èˆ¬" if score > 0.2 else "âŒ è¾ƒå·®"
        html += f"""
        <tr>
            <td>{result["id"]}</td>
            <td>{result["app"]}</td>
            <td>{score:.4f}</td>
            <td>{rating}</td>
        </tr>
"""

    html += """
    </table>

    <h2>ğŸ¨ å›¾åƒç¼–è¾‘èƒ½åŠ›è¯„ä¼°</h2>
    <table>
        <tr>
            <th>ID</th>
            <th>LPIPS Distance</th>
            <th>è¯„ä»·</th>
        </tr>
"""

    for result in eval_results.get("img2img", []):
        distance = result["lpips_distance"]
        rating = "âœ… é€‚ä¸­" if 0.1 < distance < 0.3 else "âš ï¸  è¿‡å¤§" if distance > 0.3 else "âš ï¸  è¿‡å°"
        html += f"""
        <tr>
            <td>{result["id"]}</td>
            <td>{distance:.4f}</td>
            <td>{rating}</td>
        </tr>
"""

    html += """
    </table>

    <h2>ğŸ’¡ ç»“è®ºä¸å»ºè®®</h2>
    <div class="metric">
        <h3>æ ¸å¿ƒå‘ç°</h3>
        <ul>
            <li>å¾…è¡¥å……ï¼šåŸºäºå®é™…æµ‹è¯•ç»“æœå¡«å†™</li>
            <li>æ–‡ç”Ÿå›¾èƒ½åŠ›: [ä¼˜ç§€/ä¸€èˆ¬/è¾ƒå·®]</li>
            <li>å›¾åƒç¼–è¾‘èƒ½åŠ›: [ä¼˜ç§€/ä¸€èˆ¬/è¾ƒå·®]</li>
            <li>ä¸­æ–‡æ”¯æŒ: [æ˜¯/å¦]</li>
        </ul>

        <h3>åç»­å»ºè®®</h3>
        <ul>
            <li>å¦‚æœæ•ˆæœè‰¯å¥½ â†’ è¿›å…¥LoRAå¾®è°ƒé˜¶æ®µ</li>
            <li>å¦‚æœæ•ˆæœä¸€èˆ¬ â†’ å°è¯•Fluxå¤‡é€‰æ–¹æ¡ˆ</li>
            <li>å¦‚æœæ•ˆæœè¾ƒå·® â†’ è€ƒè™‘å…¶ä»–æŠ€æœ¯è·¯çº¿</li>
        </ul>
    </div>

    <h2>ğŸ“‚ è¯¦ç»†ç»“æœ</h2>
    <p>æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ:</p>
    <ul>
        <li><a href="../outputs/text2img">æ–‡ç”Ÿå›¾ç»“æœ</a></li>
        <li><a href="../outputs/img2img">å›¾åƒç¼–è¾‘ç»“æœ</a></li>
    </ul>
</body>
</html>
"""

    output_path = "outputs/spike_report.html"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html)

    print(f"âœ… Report generated: {output_path}")
    print(f"Open in browser: file://{os.path.abspath(output_path)}")

if __name__ == "__main__":
    generate_html_report()
```

### æ‰§è¡Œè¯„ä¼°ä¸æŠ¥å‘Š

```bash
# è¿è¡Œè¯„ä¼°
python scripts/evaluate_quality.py

# ç”ŸæˆæŠ¥å‘Š
python scripts/generate_report.py

# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
# Windows: start outputs/spike_report.html
# macOS: open outputs/spike_report.html
# Linux: xdg-open outputs/spike_report.html
```

---

## æˆåŠŸæ ‡å‡†

### å¿…è¾¾æŒ‡æ ‡ (P0)

| æŒ‡æ ‡ | ç›®æ ‡ | è¯„ä¼°æ–¹æ³• |
|------|------|---------|
| æ¨¡å‹å¯è¿è¡Œ | âœ… æˆåŠŸåŠ è½½å’Œæ¨ç† | åŸºç¡€æµ‹è¯•é€šè¿‡ |
| ç”Ÿæˆé€Ÿåº¦ | < 10ç§’/å›¾ | æ€§èƒ½åŸºå‡†æµ‹è¯• |
| æ˜¾å­˜å ç”¨ | < 16GB | GPUç›‘æ§ |
| æ–‡ç”Ÿå›¾å¯ç”¨æ€§ | > 50% å›¾åƒå¯è¾¨è®¤ | äººå·¥è¯„ä¼° |

### æœŸæœ›æŒ‡æ ‡ (P1)

| æŒ‡æ ‡ | ç›®æ ‡ | è¯„ä¼°æ–¹æ³• |
|------|------|---------|
| CLIPç›¸ä¼¼åº¦ | > 0.25 | è‡ªåŠ¨è¯„ä¼° |
| ä¸­æ–‡æ”¯æŒ | èƒ½ç†è§£ä¸­æ–‡æç¤ºè¯ | å¯¹æ¯”æµ‹è¯• |
| å¼‚å¸¸å…ƒç´ å‡†ç¡®æ€§ | > 60% ç¬¦åˆè¦æ±‚ | äººå·¥è¯„ä¼° |
| ç¼–è¾‘ä¿çœŸåº¦ | LPIPSåœ¨0.1-0.3 | è‡ªåŠ¨è¯„ä¼° |

### ä¼˜ç§€æŒ‡æ ‡ (P2)

| æŒ‡æ ‡ | ç›®æ ‡ | è¯„ä¼°æ–¹æ³• |
|------|------|---------|
| UIé£æ ¼ä¸€è‡´æ€§ | è¯†åˆ«å‡ºappç‰¹å¾ | äººå·¥è¯„ä¼° |
| æ–‡å­—æ¸…æ™°åº¦ | ä¸­æ–‡å¯è¯» | äººå·¥è¯„ä¼° |
| å¸ƒå±€åˆç†æ€§ | ç¬¦åˆç§»åŠ¨ç«¯è§„èŒƒ | ä¸“å®¶è¯„å®¡ |

---

## é£é™©ä¸åº”å¯¹

### æŠ€æœ¯é£é™©

#### é£é™©1: Z-Imageæ¨¡å‹è·å–å›°éš¾

**ç°è±¡**: Hugging Faceä¸Šæ‰¾ä¸åˆ°Z-Image Turboå®˜æ–¹æ¨¡å‹

**åº”å¯¹æªæ–½**:
1. æœç´¢å…³é”®è¯: "Z-Image", "ZImage", "ZImg"
2. æŸ¥çœ‹æœ€æ–°çš„Diffusionæ¨¡å‹æ’è¡Œæ¦œ
3. è”ç³»ç›¸å…³æŠ€æœ¯ç¤¾åŒºç¡®è®¤æ¨¡å‹åç§°
4. **å¤‡é€‰æ–¹æ¡ˆ**: ä½¿ç”¨SDXL Turboæˆ–Flux.1-schnellä½œä¸ºæ›¿ä»£

```bash
# å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨SDXL Turbo
pip install diffusers transformers accelerate
python -c "
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained(
    'stabilityai/sdxl-turbo',
    torch_dtype=torch.float16
).to('cuda')
"
```

#### é£é™©2: æ˜¾å­˜ä¸è¶³

**ç°è±¡**: OOM (Out of Memory) é”™è¯¯

**åº”å¯¹æªæ–½**:
```python
# æ–¹æ¡ˆ1: é™ä½åˆ†è¾¨ç‡
height, width = 512, 512  # è€Œä¸æ˜¯768x512

# æ–¹æ¡ˆ2: å¯ç”¨CPU offload
pipe.enable_model_cpu_offload()

# æ–¹æ¡ˆ3: ä½¿ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–
pipe.enable_attention_slicing()
pipe.enable_vae_slicing()

# æ–¹æ¡ˆ4: é™ä½batch size
# é€å¼ ç”Ÿæˆï¼Œä¸è¦æ‰¹é‡

# æ–¹æ¡ˆ5: ä½¿ç”¨FP16ç”šè‡³INT8
torch_dtype = torch.float16
```

#### é£é™©3: ç”Ÿæˆè´¨é‡å·®

**ç°è±¡**: ç”Ÿæˆçš„å›¾åƒæ¨¡ç³Šã€å¤±çœŸæˆ–æ— æ³•è¯†åˆ«

**åº”å¯¹æªæ–½**:
1. **è°ƒæ•´æç¤ºè¯**:
   ```python
   # æ·»åŠ è´¨é‡æå‡è¯
   prompt = "a high-quality mobile app screenshot, " + original_prompt
   negative_prompt = "blurry, low quality, distorted, ugly, bad anatomy"
   ```

2. **å¢åŠ æ¨ç†æ­¥æ•°**:
   ```python
   num_inference_steps = 50  # ä»20å¢åŠ åˆ°50
   ```

3. **è°ƒæ•´guidance scale**:
   ```python
   guidance_scale = 9.0  # ä»7.5å¢åŠ åˆ°9
   ```

4. **å°è¯•ä¸åŒçš„Scheduler**:
   ```python
   from diffusers import DPMSolverMultistepScheduler
   pipe.scheduler = DPMSolverMultistepScheduler.from_config(
       pipe.scheduler.config
   )
   ```

### å·¥ç¨‹é£é™©

#### é£é™©4: ç¯å¢ƒé…ç½®é—®é¢˜

**åº”å¯¹æªæ–½**:
```bash
# ä½¿ç”¨Dockerå®¹å™¨éš”ç¦»ç¯å¢ƒ
docker pull pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel

docker run --gpus all -it \
  -v $(pwd):/workspace \
  pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel \
  /bin/bash
```

#### é£é™©5: æ•°æ®ä¸è¶³

**åº”å¯¹æªæ–½**:
1. ä½¿ç”¨ç½‘ç»œæœç´¢æ‰¹é‡ä¸‹è½½appæˆªå›¾
2. ä½¿ç”¨æ¨¡æ‹Ÿå™¨å½•åˆ¶appæ“ä½œè§†é¢‘ï¼Œæå–å¸§
3. ä½¿ç”¨Appiumè‡ªåŠ¨åŒ–é‡‡é›†
4. å¯»æ‰¾å…¬å¼€çš„mobile UIæ•°æ®é›†

---

## é™„å½•

### A. å®Œæ•´ä¾èµ–æ¸…å•

```bash
# requirements.txt
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
diffusers>=0.21.0
accelerate>=0.20.0
safetensors>=0.3.1
xformers>=0.0.20
Pillow>=9.5.0
opencv-python>=4.7.0
clip @ git+https://github.com/openai/CLIP.git
lpips>=0.1.4
scikit-image>=0.21.0
gradio>=3.35.0
huggingface-hub>=0.16.0
```

### B. GPUå†…å­˜ä¼˜åŒ–æŠ€å·§

```python
# å®Œæ•´çš„å†…å­˜ä¼˜åŒ–ç¤ºä¾‹
import torch
from diffusers import DiffusionPipeline

def load_optimized_pipeline(model_path):
    """åŠ è½½å†…å­˜ä¼˜åŒ–çš„pipeline"""

    pipe = DiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=torch.float16,  # ä½¿ç”¨FP16
        use_safetensors=True,
        variant="fp16",  # å¦‚æœæœ‰fp16å˜ä½“
    )

    # å¤šç§ä¼˜åŒ–é€‰é¡¹
    pipe.enable_xformers_memory_efficient_attention()  # æœ€é‡è¦
    pipe.enable_attention_slicing(1)  # æ³¨æ„åŠ›åˆ‡ç‰‡
    pipe.enable_vae_slicing()  # VAEåˆ‡ç‰‡

    # æ ¹æ®æ˜¾å­˜é€‰æ‹©
    available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

    if available_memory < 12:
        # å°æ˜¾å­˜: ä½¿ç”¨CPU offload
        pipe.enable_model_cpu_offload()
        print("Using CPU offload (æ˜¾å­˜ < 12GB)")
    else:
        # è¶³å¤Ÿæ˜¾å­˜: å…¨éƒ¨æ”¾åœ¨GPU
        pipe = pipe.to("cuda")
        print(f"All on GPU (æ˜¾å­˜ {available_memory:.1f}GB)")

    return pipe
```

### C. å¿«é€Ÿè°ƒè¯•å·¥å…·

åˆ›å»º `scripts/quick_test.py`:

```python
"""
å¿«é€Ÿæµ‹è¯•å·¥å…·
ç”¨äºå¿«é€ŸéªŒè¯å•ä¸ªåŠŸèƒ½
"""
import torch
from diffusers import DiffusionPipeline
from PIL import Image

def quick_text2img_test():
    """å¿«é€Ÿæ–‡ç”Ÿå›¾æµ‹è¯•"""
    pipe = DiffusionPipeline.from_pretrained(
        "stabilityai/sdxl-turbo",  # ä½¿ç”¨SDXL Turboä½œä¸ºå¿«é€Ÿæµ‹è¯•
        torch_dtype=torch.float16
    ).to("cuda")

    prompt = "a mobile phone screenshot showing a payment failed error"

    image = pipe(
        prompt=prompt,
        num_inference_steps=1,  # Turboåªéœ€1æ­¥
        guidance_scale=0.0,
    ).images[0]

    image.save("quick_test.png")
    print("âœ… Quick test passed! Check quick_test.png")

if __name__ == "__main__":
    quick_text2img_test()
```

### D. å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| CUDA out of memory | æ˜¾å­˜ä¸è¶³ | é™ä½åˆ†è¾¨ç‡ã€å¯ç”¨CPU offload |
| ç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢ | æœªå¯ç”¨xformers | `pip install xformers` |
| å›¾åƒæ¨¡ç³Š | æ¨ç†æ­¥æ•°å¤ªå°‘ | å¢åŠ åˆ°50æ­¥ |
| ä¸­æ–‡æç¤ºè¯æ— æ•ˆ | æ¨¡å‹ä¸æ”¯æŒä¸­æ–‡ | ä½¿ç”¨è‹±æ–‡æˆ–å¤šè¯­è¨€æ¨¡å‹ |
| æ¨¡å‹ä¸‹è½½å¤±è´¥ | ç½‘ç»œé—®é¢˜ | ä½¿ç”¨é•œåƒç«™æˆ–VPN |

---

## æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### æœ¬æ¬¡æŠ€æœ¯ç©¿åˆºäº¤ä»˜ç‰©

- âœ… å®Œæ•´çš„æµ‹è¯•è„šæœ¬ï¼ˆ6ä¸ªPythonè„šæœ¬ï¼‰
- âœ… 9ä¸ªå¼‚å¸¸åœºæ™¯çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆ3ä¸ªapp Ã— 3ä¸ªåœºæ™¯ï¼‰
- âœ… 4ä¸ªå›¾åƒç¼–è¾‘ä»»åŠ¡
- âœ… è‡ªåŠ¨åŒ–è¯„ä¼°æŠ¥å‘Š
- âœ… HTMLå¯è§†åŒ–æŠ¥å‘Š

### å†³ç­–ç‚¹

æ ¹æ®æµ‹è¯•ç»“æœï¼Œå†³å®šåç»­æ–¹å‘ï¼š

**å¦‚æœæ–‡ç”Ÿå›¾æ•ˆæœä¼˜ç§€** (CLIP > 0.3):
- âœ… ç»§ç»­ä½¿ç”¨Z-Imageä½œä¸ºä¸»æ–¹æ¡ˆ
- â†’ è¿›å…¥LoRAå¾®è°ƒé˜¶æ®µï¼ˆ2å‘¨ï¼‰
- â†’ æ‰©å±•åˆ°10+å¼‚å¸¸åœºæ™¯

**å¦‚æœæ–‡ç”Ÿå›¾æ•ˆæœä¸€èˆ¬** (CLIP 0.2-0.3):
- âš ï¸  éœ€è¦å¾®è°ƒæ‰èƒ½å®ç”¨
- â†’ æ”¶é›†è®­ç»ƒæ•°æ®ï¼ˆ500-1000å¼ /appï¼‰
- â†’ æ‰§è¡Œä¸¤é˜¶æ®µLoRAå¾®è°ƒ

**å¦‚æœæ–‡ç”Ÿå›¾æ•ˆæœè¾ƒå·®** (CLIP < 0.2):
- âŒ è€ƒè™‘å¤‡é€‰æ–¹æ¡ˆ
- â†’ æµ‹è¯•Flux 12Bé‡åŒ–ç‰ˆ
- â†’ æˆ–è°ƒæ•´æŠ€æœ¯è·¯çº¿ï¼ˆå¦‚LLM+ä»£ç ç”Ÿæˆï¼‰

**å¦‚æœå›¾åƒç¼–è¾‘æ•ˆæœä¼˜ç§€** (LPIPS 0.1-0.3):
- âœ… ä¼˜å…ˆä½¿ç”¨ç¼–è¾‘æ–¹æ¡ˆ
- â†’ æ‰¹é‡é‡‡é›†æ­£å¸¸ç•Œé¢
- â†’ é€šè¿‡ç¼–è¾‘æ³¨å…¥å¼‚å¸¸

### åç»­è®¡åˆ’

**Phase 2: LoRAå¾®è°ƒ** (å¦‚æœåŸºç¡€æµ‹è¯•é€šè¿‡):
1. æ•°æ®é‡‡é›†: æ”¶é›†1000+æ­£å¸¸æˆªå›¾/app
2. é£æ ¼å¯¹é½LoRAè®­ç»ƒï¼ˆ1-2å¤©ï¼‰
3. å¼‚å¸¸æ³¨å…¥LoRAè®­ç»ƒï¼ˆ1-2å¤©ï¼‰
4. æ•ˆæœéªŒè¯ä¸ä¼˜åŒ–

**Phase 3: ç”Ÿäº§éƒ¨ç½²**:
1. æ„å»ºæ¨ç†APIæœåŠ¡
2. å»ºç«‹è´¨é‡è¯„ä¼°pipeline
3. æ­å»ºå¼‚å¸¸åœºæ™¯åº“ï¼ˆ50+åœºæ™¯ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-01-08
**çŠ¶æ€**: å¾…æ‰§è¡Œ
**é¢„è®¡å®Œæˆ**: 2026-01-15
